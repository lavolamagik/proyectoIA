{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Nodo del Árbol de Decisión\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Implementación del Árbol de Decisión\n",
    "class DecisionTreeClassifierCustom:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Criterio de parada\n",
    "        if len(unique_labels) == 1 or depth >= self.max_depth or num_samples <= 1:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Encontrar la mejor división\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Dividir los datos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return DecisionNode(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gain = -1\n",
    "        split_index, split_threshold = None, None\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = X[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            for threshold in possible_thresholds:\n",
    "                gain = self._information_gain(y, feature_values, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature_index\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_index, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_values, threshold):\n",
    "        # Calcular la ganancia de información\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = feature_values > threshold\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_indices]), len(y[right_indices])\n",
    "\n",
    "        entropy_left = self._entropy(y[left_indices])\n",
    "        entropy_right = self._entropy(y[right_indices])\n",
    "\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Codificación de variables categóricas\n",
    "labelencoder = LabelEncoder()\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "for col in categorical_columns:\n",
    "    df[col] = labelencoder.fit_transform(df[col])\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = df.drop('NObeyesdad', axis=1).values\n",
    "y = labelencoder.fit_transform(df['NObeyesdad'])  # Codificar la variable objetivo\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo de Árbol de Decisión personalizado\n",
    "clf_custom = DecisionTreeClassifierCustom(max_depth=9)\n",
    "clf_custom.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = clf_custom.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {accuracy:.2f}')\n",
    "print('\\nMatriz de Confusión:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nReporte de Clasificación:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de Confusión para la partición actual:\n",
      "[[54  2  0  0  0  0  0]\n",
      " [ 5 46  0  0  0 10  1]\n",
      " [ 0  0 76  2  0  0  0]\n",
      " [ 0  0  2 56  0  0  0]\n",
      " [ 0  0  0  0 63  0  0]\n",
      " [ 0  3  0  0  0 43 10]\n",
      " [ 0  0  1  0  0  2 47]]\n",
      "Reporte de Clasificación para la partición actual:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94        56\n",
      "           1       0.90      0.74      0.81        62\n",
      "           2       0.96      0.97      0.97        78\n",
      "           3       0.97      0.97      0.97        58\n",
      "           4       1.00      1.00      1.00        63\n",
      "           5       0.78      0.77      0.77        56\n",
      "           6       0.81      0.94      0.87        50\n",
      "\n",
      "    accuracy                           0.91       423\n",
      "   macro avg       0.91      0.91      0.90       423\n",
      "weighted avg       0.91      0.91      0.91       423\n",
      "\n",
      "Matriz de Confusión para la partición actual:\n",
      "[[49  1  0  0  0  0  0]\n",
      " [ 3 51  0  0  0  6  0]\n",
      " [ 0  0 54  1  0  0  4]\n",
      " [ 0  0  4 57  0  0  0]\n",
      " [ 0  0  1  0 74  0  0]\n",
      " [ 0  6  0  0  0 45  5]\n",
      " [ 0  0  1  0  0  5 55]]\n",
      "Reporte de Clasificación para la partición actual:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96        50\n",
      "           1       0.88      0.85      0.86        60\n",
      "           2       0.90      0.92      0.91        59\n",
      "           3       0.98      0.93      0.96        61\n",
      "           4       1.00      0.99      0.99        75\n",
      "           5       0.80      0.80      0.80        56\n",
      "           6       0.86      0.90      0.88        61\n",
      "\n",
      "    accuracy                           0.91       422\n",
      "   macro avg       0.91      0.91      0.91       422\n",
      "weighted avg       0.91      0.91      0.91       422\n",
      "\n",
      "Matriz de Confusión para la partición actual:\n",
      "[[55  4  0  0  0  0  0]\n",
      " [ 5 61  0  0  0  1  4]\n",
      " [ 0  0 66  1  0  0  6]\n",
      " [ 0  0  1 60  0  0  0]\n",
      " [ 0  0  0  0 47  0  0]\n",
      " [ 0  4  0  0  0 36 18]\n",
      " [ 0  0  0  0  0  1 52]]\n",
      "Reporte de Clasificación para la partición actual:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92        59\n",
      "           1       0.88      0.86      0.87        71\n",
      "           2       0.99      0.90      0.94        73\n",
      "           3       0.98      0.98      0.98        61\n",
      "           4       1.00      1.00      1.00        47\n",
      "           5       0.95      0.62      0.75        58\n",
      "           6       0.65      0.98      0.78        53\n",
      "\n",
      "    accuracy                           0.89       422\n",
      "   macro avg       0.91      0.90      0.89       422\n",
      "weighted avg       0.91      0.89      0.89       422\n",
      "\n",
      "Matriz de Confusión para la partición actual:\n",
      "[[55  0  0  0  0  0  0]\n",
      " [ 1 40  0  0  0  6  0]\n",
      " [ 0  0 64  1  0  0  6]\n",
      " [ 0  0  2 48  1  0  0]\n",
      " [ 0  0  0  0 60  0  0]\n",
      " [ 0  5  0  0  0 62  1]\n",
      " [ 0  0  4  0  0  6 60]]\n",
      "Reporte de Clasificación para la partición actual:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        55\n",
      "           1       0.89      0.85      0.87        47\n",
      "           2       0.91      0.90      0.91        71\n",
      "           3       0.98      0.94      0.96        51\n",
      "           4       0.98      1.00      0.99        60\n",
      "           5       0.84      0.91      0.87        68\n",
      "           6       0.90      0.86      0.88        70\n",
      "\n",
      "    accuracy                           0.92       422\n",
      "   macro avg       0.93      0.92      0.92       422\n",
      "weighted avg       0.92      0.92      0.92       422\n",
      "\n",
      "Matriz de Confusión para la partición actual:\n",
      "[[51  1  0  0  0  0  0]\n",
      " [ 0 43  0  0  0  3  1]\n",
      " [ 0  0 67  0  0  0  3]\n",
      " [ 0  0  1 65  0  0  0]\n",
      " [ 0  0  0  0 79  0  0]\n",
      " [ 0  2  0  0  0 47  3]\n",
      " [ 0  0  5  0  0  0 51]]\n",
      "Reporte de Clasificación para la partición actual:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        52\n",
      "           1       0.93      0.91      0.92        47\n",
      "           2       0.92      0.96      0.94        70\n",
      "           3       1.00      0.98      0.99        66\n",
      "           4       1.00      1.00      1.00        79\n",
      "           5       0.94      0.90      0.92        52\n",
      "           6       0.88      0.91      0.89        56\n",
      "\n",
      "    accuracy                           0.95       422\n",
      "   macro avg       0.95      0.95      0.95       422\n",
      "weighted avg       0.96      0.95      0.96       422\n",
      "\n",
      "Precisión promedio del modelo con validación cruzada: 0.92\n",
      "Desviación estándar de la precisión: 0.02\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Nodo del Árbol de Decisión\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Implementación del Árbol de Decisión\n",
    "class DecisionTreeClassifierCustom:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Criterio de parada\n",
    "        if len(unique_labels) == 1 or depth >= self.max_depth or num_samples < self.min_samples_split or num_samples <= self.min_samples_leaf:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Encontrar la mejor división\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Dividir los datos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return DecisionNode(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gain = -1\n",
    "        split_index, split_threshold = None, None\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = X[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            for threshold in possible_thresholds:\n",
    "                gain = self._information_gain(y, feature_values, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature_index\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_index, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_values, threshold):\n",
    "        # Calcular la ganancia de información\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = feature_values > threshold\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_indices]), len(y[right_indices])\n",
    "\n",
    "        entropy_left = self._entropy(y[left_indices])\n",
    "        entropy_right = self._entropy(y[right_indices])\n",
    "\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Codificación de variables categóricas\n",
    "labelencoder = LabelEncoder()\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "for col in categorical_columns:\n",
    "    df[col] = labelencoder.fit_transform(df[col])\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = df.drop('NObeyesdad', axis=1).values\n",
    "y = labelencoder.fit_transform(df['NObeyesdad'])  # Codificar la variable objetivo\n",
    "\n",
    "# Validación cruzada para evaluar el modelo\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Entrenar el modelo de Árbol de Decisión personalizado\n",
    "    clf_custom = DecisionTreeClassifierCustom(max_depth=7, min_samples_split=5, min_samples_leaf=5)\n",
    "    clf_custom.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    y_pred = clf_custom.predict(X_test)\n",
    "\n",
    "    # Evaluación del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Imprimir la matriz de confusión y el reporte de clasificación para cada partición\n",
    "    print(f\"Matriz de Confusión para la partición actual:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"Reporte de Clasificación para la partición actual:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "# Imprimir los resultados de validación cruzada\n",
    "print(f'Precisión promedio del modelo con validación cruzada: {np.mean(accuracies):.2f}')\n",
    "print(f'Desviación estándar de la precisión: {np.std(accuracies):.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83  3  0  0  7  0  0]\n",
      " [ 1 86  1  0  0  0  0]\n",
      " [ 0  4 74  1  0  0  0]\n",
      " [ 0  0  0 99  0  3  0]\n",
      " [ 4  0  0  0 82  0  0]\n",
      " [ 0  0  0  6  0 82  0]\n",
      " [ 0  0  0  1  0  0 97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.92        93\n",
      "           1       0.92      0.98      0.95        88\n",
      "           2       0.99      0.94      0.96        79\n",
      "           3       0.93      0.97      0.95       102\n",
      "           4       0.92      0.95      0.94        86\n",
      "           5       0.96      0.93      0.95        88\n",
      "           6       1.00      0.99      0.99        98\n",
      "\n",
      "    accuracy                           0.95       634\n",
      "   macro avg       0.95      0.95      0.95       634\n",
      "weighted avg       0.95      0.95      0.95       634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Clase para el Árbol de Decisión\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.c_[X, y]\n",
    "        self.tree = self._build_tree(dataset)\n",
    "\n",
    "    def _build_tree(self, dataset, depth=0):\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Obtener mejor división\n",
    "        best_feature, best_threshold = self._best_split(dataset, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Crear nodos hijos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_child = self._build_tree(dataset[left_indices], depth + 1)\n",
    "        right_child = self._build_tree(dataset[right_indices], depth + 1)\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_child, \"right\": right_child}\n",
    "\n",
    "    def _best_split(self, dataset, num_features):\n",
    "        best_gain = -float(\"inf\")\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(dataset[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(dataset, feature, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, dataset, feature, threshold):\n",
    "        parent_entropy = self._entropy(dataset[:, -1])\n",
    "\n",
    "        left_split = dataset[dataset[:, feature] <= threshold]\n",
    "        right_split = dataset[dataset[:, feature] > threshold]\n",
    "\n",
    "        if len(left_split) == 0 or len(right_split) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(dataset)\n",
    "        n_left, n_right = len(left_split), len(right_split)\n",
    "        child_entropy = (n_left / n) * self._entropy(left_split[:, -1]) + (n_right / n) * self._entropy(right_split[:, -1])\n",
    "\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y.astype(int))\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "            return self._traverse_tree(x, node[\"left\"])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node[\"right\"])\n",
    "\n",
    "# Clase para el Random Forest\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            # Crear un árbol y entrenarlo\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Votación mayoritaria\n",
    "        y_pred = [Counter(tree_preds[:, i]).most_common(1)[0][0] for i in range(X.shape[0])]\n",
    "        return y_pred\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Preprocesamiento: codificación de variables categóricas\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS', ]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separación de características y etiquetas\n",
    "X = df.drop('NObeyesdad', axis=1).values  # Características\n",
    "y = df['NObeyesdad'].factorize()[0]  # Etiqueta como numérico\n",
    "\n",
    "# División del dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear el modelo Random Forest desde cero\n",
    "rf_custom = RandomForest(n_estimators=10, max_depth=10, min_samples_split=2)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_custom.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_custom.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sin train mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[81  6  0  0  6  0  0]\n",
      " [ 3 84  1  0  0  0  0]\n",
      " [ 0  3 76  0  0  0  0]\n",
      " [ 0  2  0 97  0  3  0]\n",
      " [ 6  0  0  0 80  0  0]\n",
      " [ 0  0  0  2  0 85  0]\n",
      " [ 0  0  0  1  0  0 97]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89        93\n",
      "           1       0.88      0.95      0.92        88\n",
      "           2       0.99      0.96      0.97        79\n",
      "           3       0.97      0.95      0.96       102\n",
      "           4       0.93      0.93      0.93        86\n",
      "           5       0.97      0.98      0.97        87\n",
      "           6       1.00      0.99      0.99        98\n",
      "\n",
      "    accuracy                           0.95       633\n",
      "   macro avg       0.95      0.95      0.95       633\n",
      "weighted avg       0.95      0.95      0.95       633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Clase para el Árbol de Decisión\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.c_[X, y]\n",
    "        self.tree = self._build_tree(dataset)\n",
    "\n",
    "    def _build_tree(self, dataset, depth=0):\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Obtener mejor división\n",
    "        best_feature, best_threshold = self._best_split(dataset, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Crear nodos hijos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_child = self._build_tree(dataset[left_indices], depth + 1)\n",
    "        right_child = self._build_tree(dataset[right_indices], depth + 1)\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_child, \"right\": right_child}\n",
    "\n",
    "    def _best_split(self, dataset, num_features):\n",
    "        best_gain = -float(\"inf\")\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(dataset[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(dataset, feature, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, dataset, feature, threshold):\n",
    "        parent_entropy = self._entropy(dataset[:, -1])\n",
    "\n",
    "        left_split = dataset[dataset[:, feature] <= threshold]\n",
    "        right_split = dataset[dataset[:, feature] > threshold]\n",
    "\n",
    "        if len(left_split) == 0 or len(right_split) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(dataset)\n",
    "        n_left, n_right = len(left_split), len(right_split)\n",
    "        child_entropy = (n_left / n) * self._entropy(left_split[:, -1]) + (n_right / n) * self._entropy(right_split[:, -1])\n",
    "\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y.astype(int))\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "            return self._traverse_tree(x, node[\"left\"])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node[\"right\"])\n",
    "\n",
    "# Clase para el Random Forest\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Bootstrap sampling\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample = X[indices]\n",
    "            y_sample = y[indices]\n",
    "\n",
    "            # Crear un árbol y entrenarlo\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Votación mayoritaria\n",
    "        y_pred = [Counter(tree_preds[:, i]).most_common(1)[0][0] for i in range(X.shape[0])]\n",
    "        return y_pred\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Preprocesamiento: codificación de variables categóricas\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Separación de características y etiquetas\n",
    "X = df.drop('NObeyesdad', axis=1).values  # Características\n",
    "y = df['NObeyesdad'].factorize()[0]  # Etiqueta como numérico\n",
    "\n",
    "# Implementación manual de la división del dataset en entrenamiento y prueba\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "test_set_size = int(len(X) * 0.3)\n",
    "test_indices = shuffled_indices[:test_set_size]\n",
    "train_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Crear el modelo Random Forest desde cero\n",
    "rf_custom = RandomForest(n_estimators=10, max_depth=10, min_samples_split=5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_custom.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = rf_custom.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# árbol de decicion todo implementado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluación con Árbol de Decisión único:\n",
      "Matriz de Confusión (Árbol de Decisión):\n",
      "[[74 12  0  0  7  0  0]\n",
      " [ 2 85  1  0  0  0  0]\n",
      " [ 0  4 74  1  0  0  0]\n",
      " [ 0  0  1 98  0  3  0]\n",
      " [ 2  0  0  0 84  0  0]\n",
      " [ 0  0  0  5  0 82  0]\n",
      " [ 0  0  0  1  0  0 97]]\n",
      "Clase 0 - Precision: 0.95, Recall: 0.80, F1-Score: 0.87\n",
      "Clase 1 - Precision: 0.84, Recall: 0.97, F1-Score: 0.90\n",
      "Clase 2 - Precision: 0.97, Recall: 0.94, F1-Score: 0.95\n",
      "Clase 3 - Precision: 0.93, Recall: 0.96, F1-Score: 0.95\n",
      "Clase 4 - Precision: 0.92, Recall: 0.98, F1-Score: 0.95\n",
      "Clase 5 - Precision: 0.96, Recall: 0.94, F1-Score: 0.95\n",
      "Clase 6 - Precision: 1.00, Recall: 0.99, F1-Score: 0.99\n",
      "F1-Score Promedio: 0.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Clase para el Árbol de Decisión\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        dataset = np.c_[X, y]\n",
    "        self.tree = self._build_tree(dataset)\n",
    "\n",
    "    def _build_tree(self, dataset, depth=0):\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        if (depth >= self.max_depth or num_samples < self.min_samples_split or len(set(y)) == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Obtener mejor división\n",
    "        best_feature, best_threshold = self._best_split(dataset, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return leaf_value\n",
    "\n",
    "        # Crear nodos hijos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left_child = self._build_tree(dataset[left_indices], depth + 1)\n",
    "        right_child = self._build_tree(dataset[right_indices], depth + 1)\n",
    "        return {\"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_child, \"right\": right_child}\n",
    "\n",
    "    def _best_split(self, dataset, num_features):\n",
    "        best_gain = -float(\"inf\")\n",
    "        best_feature, best_threshold = None, None\n",
    "        for feature in range(num_features):\n",
    "            thresholds = np.unique(dataset[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(dataset, feature, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain, best_feature, best_threshold = gain, feature, threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, dataset, feature, threshold):\n",
    "        parent_entropy = self._entropy(dataset[:, -1])\n",
    "\n",
    "        left_split = dataset[dataset[:, feature] <= threshold]\n",
    "        right_split = dataset[dataset[:, feature] > threshold]\n",
    "\n",
    "        if len(left_split) == 0 or len(right_split) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(dataset)\n",
    "        n_left, n_right = len(left_split), len(right_split)\n",
    "        child_entropy = (n_left / n) * self._entropy(left_split[:, -1]) + (n_right / n) * self._entropy(right_split[:, -1])\n",
    "\n",
    "        info_gain = parent_entropy - child_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y.astype(int))\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)\n",
    "        return counter.most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if not isinstance(node, dict):\n",
    "            return node\n",
    "\n",
    "        if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "            return self._traverse_tree(x, node[\"left\"])\n",
    "        else:\n",
    "            return self._traverse_tree(x, node[\"right\"])\n",
    "\n",
    "# Implementación de codificación manual de variables categóricas\n",
    "def manual_label_encoder(column):\n",
    "    unique_values = list(set(column))\n",
    "    value_to_int = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    return [value_to_int[val] for val in column]\n",
    "\n",
    "# Implementación manual de la matriz de confusión\n",
    "def confusion_matrix_manual(y_true, y_pred):\n",
    "    unique_classes = np.unique(y_true)\n",
    "    matrix = np.zeros((len(unique_classes), len(unique_classes)), dtype=int)\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        matrix[int(true)][int(pred)] += 1\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Implementación manual de precision, recall, f1-score\n",
    "def precision_recall_f1(cm):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(len(cm)):\n",
    "        tp = cm[i, i]\n",
    "        fp = sum(cm[:, i]) - tp\n",
    "        fn = sum(cm[i, :]) - tp\n",
    "\n",
    "        # Precision, Recall, F1-Score\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return precisions, recalls, f1_scores\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Preprocesamiento: codificación de variables categóricas\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = manual_label_encoder(df[col])\n",
    "\n",
    "# Codificación de la columna objetivo 'NObeyesdad'\n",
    "df['NObeyesdad'] = manual_label_encoder(df['NObeyesdad'])\n",
    "\n",
    "# Manejo de valores faltantes (solo en columnas numéricas)\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())\n",
    "\n",
    "# Detectar y manejar valores anómalos usando el rango intercuartílico (IQR)\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_columns:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "\n",
    "# Normalización de los datos\n",
    "for col in numeric_columns:\n",
    "    df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "# Separación de características y etiquetas\n",
    "X = df.drop('NObeyesdad', axis=1).values  # Características\n",
    "y = df['NObeyesdad'].factorize()[0]  # Etiqueta como numérico\n",
    "\n",
    "# Implementación manual de la división del dataset en entrenamiento y prueba\n",
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(X))\n",
    "test_set_size = int(len(X) * 0.3)\n",
    "test_indices = shuffled_indices[:test_set_size]\n",
    "train_indices = shuffled_indices[test_set_size:]\n",
    "\n",
    "X_train, X_test = X[train_indices], X[test_indices]\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Entrenar y evaluar un solo Árbol de Decisión\n",
    "print(\"\\nEvaluación con Árbol de Decisión único:\")\n",
    "dt_custom = DecisionTree(max_depth=10, min_samples_split=5)\n",
    "dt_custom.fit(X_train, y_train)\n",
    "y_pred_dt = dt_custom.predict(X_test)\n",
    "\n",
    "# Evaluación del Árbol de Decisión\n",
    "cm_dt = confusion_matrix_manual(y_test, y_pred_dt)\n",
    "precisions_dt, recalls_dt, f1_scores_dt = precision_recall_f1(cm_dt)\n",
    "\n",
    "print(\"Matriz de Confusión (Árbol de Decisión):\")\n",
    "print(cm_dt)\n",
    "\n",
    "f1_score_avrg = 0 \n",
    "for i, (p, r, f1) in enumerate(zip(precisions_dt, recalls_dt, f1_scores_dt)):\n",
    "    f1_score_avrg += f1\n",
    "    print(f\"Clase {i} - Precision: {p:.2f}, Recall: {r:.2f}, F1-Score: {f1:.2f}\")\n",
    "\n",
    "f1_score_avrg /= len(precisions_dt)\n",
    "print(f\"F1-Score Promedio: {f1_score_avrg:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# árbol de decision pero con mas parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.96\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[53  3  0  0  0  0  0]\n",
      " [ 5 54  0  0  0  3  0]\n",
      " [ 0  0 76  2  0  0  0]\n",
      " [ 0  0  2 56  0  0  0]\n",
      " [ 0  0  0  0 63  0  0]\n",
      " [ 0  0  0  0  0 55  1]\n",
      " [ 0  0  1  0  0  2 47]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        56\n",
      "           1       0.95      0.87      0.91        62\n",
      "           2       0.96      0.97      0.97        78\n",
      "           3       0.97      0.97      0.97        58\n",
      "           4       1.00      1.00      1.00        63\n",
      "           5       0.92      0.98      0.95        56\n",
      "           6       0.98      0.94      0.96        50\n",
      "\n",
      "    accuracy                           0.96       423\n",
      "   macro avg       0.95      0.95      0.95       423\n",
      "weighted avg       0.96      0.96      0.95       423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Nodo del Árbol de Decisión\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "\n",
    "# Implementación del Árbol de Decisión\n",
    "class DecisionTreeClassifierCustom:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Criterio de parada\n",
    "        if len(unique_labels) == 1 or depth >= self.max_depth or num_samples < self.min_samples_split or num_samples <= self.min_samples_leaf:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Encontrar la mejor división\n",
    "        best_feature, best_threshold = self._best_split(X, y, num_features)\n",
    "\n",
    "        if best_feature is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        # Dividir los datos\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionNode(value=leaf_value)\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return DecisionNode(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y, num_features):\n",
    "        best_gain = -1\n",
    "        split_index, split_threshold = None, None\n",
    "\n",
    "        for feature_index in range(num_features):\n",
    "            feature_values = X[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            for threshold in possible_thresholds:\n",
    "                gain = self._information_gain(y, feature_values, threshold)\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    split_index = feature_index\n",
    "                    split_threshold = threshold\n",
    "\n",
    "        return split_index, split_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_values, threshold):\n",
    "        # Calcular la ganancia de información\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = feature_values > threshold\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left, n_right = len(y[left_indices]), len(y[right_indices])\n",
    "\n",
    "        entropy_left = self._entropy(y[left_indices])\n",
    "        entropy_right = self._entropy(y[right_indices])\n",
    "\n",
    "        child_entropy = (n_left / n) * entropy_left + (n_right / n) * entropy_right\n",
    "\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
    "\n",
    "# Codificación de variables categóricas\n",
    "labelencoder = LabelEncoder()\n",
    "categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "for col in categorical_columns:\n",
    "    df[col] = labelencoder.fit_transform(df[col])\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = df.drop('NObeyesdad', axis=1).values\n",
    "y = labelencoder.fit_transform(df['NObeyesdad'])  # Codificar la variable objetivo\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo de Árbol de Decisión personalizado\n",
    "clf_custom = DecisionTreeClassifierCustom(max_depth=9, min_samples_split=6, min_samples_leaf=4)\n",
    "clf_custom.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones\n",
    "y_pred = clf_custom.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {accuracy:.2f}')\n",
    "print('\\nMatriz de Confusión:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nReporte de Clasificación:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
